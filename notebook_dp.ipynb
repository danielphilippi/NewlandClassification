{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "from pandas_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definitions \n",
    "datain_path = 'data/'\n",
    "\n",
    "explorations_path = 'explorations/'\n",
    "if not os.path.exists(explorations_path): \n",
    "    os.makedirs(explorations_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'train': 'Train.xlsx',\n",
    "    'test':'Test.xlsx', \n",
    "    'both': {\n",
    "        'train': 'Train.xlsx',   \n",
    "        'test':'Test.xlsx',\n",
    "    } \n",
    "}\n",
    "\n",
    "#datasets = pd.DataFrame(datasets, columns=['name', 'path']).set_index('name')\n",
    "\n",
    "dataset_name = 'train'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset_name == 'both': \n",
    "    data = pd.DataFrame()\n",
    "    for dataset_path in datasets[dataset_name].values(): \n",
    "        tmp = pd.read_excel(os.path.join(datain_path, dataset_path))\n",
    "        data = pd.concat([data, tmp])\n",
    "\n",
    "else:    \n",
    "    dataset_path = datasets[dataset_name]\n",
    "    data = pd.read_excel(os.path.join(datain_path, dataset_path))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## profile report\n",
    "\n",
    "profile = ProfileReport(\n",
    "    data,\n",
    "    title='Raw data',\n",
    "    minimal=False, \n",
    "    correlations={\n",
    "    \"pearson\": {\"calculate\": True},\n",
    "    \"spearman\": {\"calculate\": False},\n",
    "    \"kendall\": {\"calculate\": False},\n",
    "    \"phi_k\": {\"calculate\": False},\n",
    "    \"cramers\": {\"calculate\": False},\n",
    "    }\n",
    ")\n",
    "profile.to_file(os.path.join(explorations_path, 'profile_data_raw.html'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feature=10\n",
    "\n",
    "def get_feature_imp_by_expl(data, base_col, n_feature=n_feature): \n",
    "    \n",
    "    # make sure every combination of levels exist, fill with 0 if no obs\n",
    "    base = data[base_col].unique()\n",
    "    Income = [0, 1]\n",
    "    idx = pd.MultiIndex.from_product(\n",
    "        [base, Income],\n",
    "        names=[base_col, 'Income']\n",
    "    )\n",
    "\n",
    "    pd1 = pd.DataFrame(index=idx)\n",
    "    \n",
    "\n",
    "    a = data.groupby([base_col, 'Income']).size().to_frame().rename(columns={0:'nobs'})\n",
    "    \n",
    "    a = pd.concat([pd1, a], axis=1)\n",
    "    a.loc[a.nobs.isna(), 'nobs'] = 0\n",
    "    \n",
    "    a['nobs_rel'] = a.groupby(level=base_col).transform(lambda x: x / (x[0] + x[1]))\n",
    "    value_cols = a.columns.to_list()\n",
    "    a.reset_index(inplace=True)\n",
    "    \n",
    "    # top Features by nobs: \n",
    "    topFeat = data.groupby(base_col).size().to_frame().rename(columns={0:'nobs'})\\\n",
    "        .sort_values('nobs', ascending=False).iloc[0:n_feature,:]\\\n",
    "        .index.to_list()\n",
    "    \n",
    "    a.sort_values(['nobs', base_col], ascending=False, inplace=True)\n",
    "    \n",
    "    #print('len(topFeat)', len(topFeat))\n",
    "    #print('len(a)', len(a))\n",
    "    #print('n_feature', n_feature)\n",
    "    #print('a[base_col].nunique()', a[base_col].nunique())\n",
    "\n",
    "    if len(topFeat) < a[base_col].nunique(): \n",
    "        print(f'***Features Filtered to top_{n_feature} by nobs!***')\n",
    "    #print(a[base_col].nunique())\n",
    "\n",
    "    a = a.loc[a[base_col].isin(topFeat),:]\n",
    "    return a, value_cols\n",
    "\n",
    "def plot_feature_imp_by_expl(data, base_col): \n",
    "    \n",
    "    a, value_cols = get_feature_imp_by_expl(data, base_col)\n",
    "    n_plots = len(value_cols) + 1\n",
    "\n",
    "    fig, ax = plt.subplots(ncols = n_plots  , figsize=(20,7), gridspec_kw={'width_ratios': [3,3,1]})\n",
    "    for i, col in enumerate(value_cols): \n",
    "        sns.barplot(data=a, x=base_col, y=col, hue='Income', ax=ax[i])#.set_title(col) # [0:10]\n",
    "        ax[i].tick_params(labelrotation=45)\n",
    "\n",
    "    sns.countplot(data=data, x='Income', ax=ax[n_plots-1])\n",
    "    plt.show()\n",
    "    \n",
    "def get_target_ratio(data):  \n",
    "    a = data.groupby(['Income']).size().to_frame().rename(columns={0:'nobs'})\n",
    "    a['nobs_rel'] = a.transform(lambda x: x / (x[0] + x[1]))\n",
    "    value_cols = a.columns.to_list()\n",
    "    a.reset_index(inplace=True)\n",
    "    return a.loc[a.Income == 1, 'nobs_rel'].to_list()[0]\n",
    "\n",
    "\n",
    "def get_feature_imp_by_target_ratio(data, base_col, weighted=False): \n",
    "\n",
    "    target_ratio = get_target_ratio(data)\n",
    "    target_ratio\n",
    "                                            \n",
    "    a, _ = get_feature_imp_by_expl(data, base_col, n_feature=100)\n",
    "    \n",
    "    #########\n",
    "    nObsPerFeatClass =  data.groupby([base_col]).size().to_frame().rename(columns={0:'nobs'})\n",
    "                           \n",
    "\n",
    "    ratio_per_level = a.loc[a.Income == 1, [base_col,'nobs_rel']]\\\n",
    "        .set_index(base_col)\\\n",
    "        .rename(columns={'nobs_rel':'class1_ratio'})\n",
    "\n",
    "    ratio_per_level = pd.concat([ratio_per_level, nObsPerFeatClass], axis=1)\n",
    "    #min_max_scaler_obs = MinMaxScaler()\n",
    "    ratio_per_level['nobs_rel'] = ratio_per_level.nobs / sum(ratio_per_level.nobs)\n",
    "\n",
    "\n",
    "    \n",
    "    ratio_per_level['diff_to_target'] = ratio_per_level['class1_ratio'] - target_ratio\n",
    "    ratio_per_level['diff_to_target_dir'] = ['neg' if obs < 0 else 'pos' for obs in ratio_per_level['diff_to_target']]\n",
    "    \n",
    "    if weighted: \n",
    "        weights = np.power(ratio_per_level['nobs_rel'], 1./3)\n",
    "    else: \n",
    "        weights = 1\n",
    "        \n",
    "    ratio_per_level['diff_to_target_abs'] = abs(ratio_per_level['diff_to_target']) * weights\n",
    "    \n",
    "    #print(ratio_per_level)\n",
    "\n",
    "    ratio_per_level.sort_values('diff_to_target_abs', ascending=False, inplace=True)\n",
    "    ratio_per_level['diff_to_target_abs_cumsum'] = ratio_per_level.diff_to_target_abs.cumsum()\n",
    "    ratio_per_level\n",
    "\n",
    "\n",
    "    x = ratio_per_level['diff_to_target_abs_cumsum'].values.reshape(-1, 1) #df.values #returns a numpy array\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    ratio_per_level['diff_to_target_abs_cumsum_scaled'] = min_max_scaler.fit_transform(x)\n",
    "\n",
    "    print('TargetClass1_ratio', target_ratio)\n",
    "    print(ratio_per_level[['diff_to_target_dir', 'diff_to_target_abs']])\n",
    "\n",
    "    return ratio_per_level\n",
    "\n",
    "\n",
    "def plot_feature_imp_by_target_ratio(data, base_col, weighted=False): \n",
    "\n",
    "    r = get_feature_imp_by_target_ratio(data, base_col, weighted)\n",
    "\n",
    "    sns.lineplot(data=r, y=r.index, x='diff_to_target_abs_cumsum_scaled')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_feature_imp_by_tree(data, base_col, n_feature=n_feature): \n",
    "    # prepare\n",
    "    onehot = OneHotEncoder()\n",
    "    X_train_cat = data.loc[:,[base_col]]\n",
    "    #X_train_cat = data[base_col]\n",
    "\n",
    "    X_train_onehot = onehot.fit_transform(X_train_cat)\n",
    "    X_train_onehot_df = pd.DataFrame(X_train_onehot.toarray(), columns=onehot.get_feature_names())\n",
    "    X_train_onehot_df\n",
    "\n",
    "    X_train_onehot_df = pd.get_dummies(data[base_col], prefix=base_col)\n",
    "\n",
    "    # train\n",
    "    dt_gini = DecisionTreeClassifier(random_state = 1)\n",
    "    X_train = X_train_onehot_df#.drop(columns=['x0_Africa','x0_Europe', 'x0_Oceania'])\n",
    "    y_train = data.Income\n",
    "\n",
    "\n",
    "    dt_gini.fit(X_train, y_train) # data[base_col]\n",
    "    print('Score:', dt_gini.score(X_train, y_train))\n",
    "\n",
    "    #dt_gini.feature_importances_\n",
    "    #tree.plot_tree(dt_gini)\n",
    "\n",
    "    #plt.barh(onehot.get_feature_names(), dt_gini.feature_importances_)\n",
    "\n",
    "    #print(dt_gini.feature_importances_)\n",
    "    sorted_idx = dt_gini.feature_importances_.argsort()#[0:10]\n",
    "    plotdata = pd.DataFrame({\n",
    "        'Feature': X_train.columns[sorted_idx], \n",
    "        'Importance': dt_gini.feature_importances_[sorted_idx]}).sort_values('Importance', ascending=False)\n",
    "    #plt.barh()\n",
    "    #print(plotdata)\n",
    "    sns.barplot(data=plotdata.iloc[0:n_feature,:], x='Importance', y='Feature')\n",
    "    plt.xlabel(\"Feature Importance\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_feature_imp(data, base_col, force_barplot=True, weighted=False): \n",
    "    print('Class distributions')\n",
    "    if (data[base_col].nunique() < 6) | force_barplot:\n",
    "        n_plots = 2\n",
    "        plot_feature_imp_by_expl(data, base_col)\n",
    "    else: \n",
    "        n_plots = 1\n",
    "        \n",
    "    print('\\nElbow')        \n",
    "    plot_feature_imp_by_target_ratio(data, base_col, weighted)\n",
    "    \n",
    "    print('\\nDecision Tree')\n",
    "    plot_feature_imp_by_tree(data, base_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init \n",
    "cols_to_drop = []\n",
    "cols_to_onehot = []\n",
    "\n",
    "# prep\n",
    "pred_config = {\n",
    "    'cardinality': 'original' # low, medium, high, original\n",
    "} \n",
    "\n",
    "cardinality = pred_config['cardinality']\n",
    "print('cardinality:', cardinality)\n",
    "\n",
    "error_log = {'cleaning': []}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract gender from name?!\n",
    "salutation = data.Name.str.split(' ', n=1, expand=True)[0]\n",
    "if salutation.nunique() != 3: \n",
    "    raise ValueError('Unexpected levels of salutation')\n",
    "    \n",
    "print(salutation.value_counts())\n",
    "\n",
    "#gender = ['male' if s == 'Mr.' else 'female' for s in salutation]\n",
    "#data['gender'] = gender\n",
    "\n",
    "male = [1 if s == 'Mr.' else 0 if s in ['Mrs.', 'Miss'] else np.nan for s in salutation]\n",
    "data['male'] = male\n",
    "\n",
    "if data.male.isna().sum() > 0: \n",
    "    raise Warning('NAs instroduced')\n",
    "\n",
    "\n",
    "sns.countplot(data=data, hue=data.Income, x='male')#.set_title(col)\n",
    "plt.show()\n",
    "    \n",
    "cols_to_drop.append('Name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute age from Birthday\n",
    "\n",
    "# clean whitespaces\n",
    "data.Birthday = data.Birthday.str.replace(' ', '')\n",
    "# define date format\n",
    "dob_format = '%B%d,%Y'\n",
    "\n",
    "# transform Birthday to datetime, catching the leap year error \n",
    "\n",
    "## helper fct to subtract one day from datetime if error occurs\n",
    "def subone(obj):\n",
    "    val = int(obj.group(0))\n",
    "    return str(val-1)\n",
    "\n",
    "## init and loop over dates\n",
    "dob = []\n",
    "warn_log = []\n",
    "for i, d in enumerate(data.Birthday): \n",
    "    try: \n",
    "        dob.append(datetime.strptime(d, dob_format).date())\n",
    "\n",
    "    except ValueError as e: \n",
    "        if str(e) == 'day is out of range for month': \n",
    "            dt = datetime.strptime(re.sub('\\d{1,2}', subone, d, count=1), dob_format).date()\n",
    "            warn_log.append((d, dt))\n",
    "            dob.append(dt)\n",
    "        else: \n",
    "            raise NotImplementedError('Do not know how to deal with that error!')\n",
    "            dt = np.nan\n",
    "            warn_log.append((d, dt))\n",
    "            dob.append(dt)\n",
    "        \n",
    "# add age column \n",
    "data['age'] = [np.floor((datetime.strptime('2048-12-31', '%Y-%m-%d').date() - d).days / 365.2425) for d in dob]\n",
    "\n",
    "# inspect\n",
    "sns.histplot(data, x='age')\n",
    "plt.show()\n",
    "print('Min age:' , min(data.age))\n",
    "\n",
    "# drop date col \n",
    "cols_to_drop.append('Birthday')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 'Native Continent' to bin \n",
    "base_col = 'Native Continent'\n",
    "#sns.countplot(data=data, hue=data.Income, x=base_col)#.set_title(col)\n",
    "#plt.show()\n",
    "\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality in ['low', 'medium']:\n",
    "        target_col = 'from_europe_or_asia'\n",
    "        #data['from_europe'] = [1 if a == 'Europe' else 0 for a in data[base_col]]\n",
    "        data[target_col] = [1 if a in ['Europe', 'Asia'] else 0 for a in data[base_col]]\n",
    "    elif cardinality == 'original':\n",
    "        target_col = 'native_continent'\n",
    "        data[target_col] = data[base_col]\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Marital Status\n",
    "base_col = 'Marital Status'\n",
    "#target_col = 'marital_status'\n",
    "\n",
    "data[base_col].value_counts()\n",
    "\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "try: \n",
    "    if cardinality == 'low': \n",
    "        target_col = 'maritalStatus_married'\n",
    "        data[target_col] = [1 if a in ['Married', 'Married - Spouse in the Army'] else 0 for a in data[base_col]]\n",
    "        \n",
    "    elif cardinality == 'medium': \n",
    "        target_col = 'maritalStatus'\n",
    "        mapping = {\n",
    "            'Married':'Married',\n",
    "            'Single':'Single',\n",
    "            'Divorced':'Divorced',\n",
    "            'Separated':'Separated',\n",
    "            'Widow':'Widow',\n",
    "            'Married - Spouse Missing':'SpouseMissing',\n",
    "            'Married - Spouse in the Army':'Married'\n",
    "        }\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "        \n",
    "    elif cardinality == 'original': \n",
    "        target_col = 'maritalStatus'\n",
    "        mapping = {\n",
    "            'Married':'Married',\n",
    "            'Single':'Single',\n",
    "            'Divorced':'Divorced',\n",
    "            'Separated':'Separated',\n",
    "            'Widow':'Widow',\n",
    "            'Married - Spouse Missing':'SpouseMissing',\n",
    "            'Married - Spouse in the Army':'MarriedArmy'\n",
    "        }\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "    \n",
    "#sns.countplot(data=data, x=target_col)\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lives with\n",
    "base_col = 'Lives with'\n",
    "print(data[base_col].value_counts())\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "try: \n",
    "    if(cardinality == 'low'): \n",
    "        target_col = 'household_livesWithPartner'\n",
    "        data[target_col] = [1 if a in ['Wife', 'Husband'] else 0 for a in data[base_col]]\n",
    "    elif(cardinality == 'medium'): \n",
    "        target_col = 'household'\n",
    "        mapping = {\n",
    "            'Wife': 'Partner',\n",
    "            'Other Family': 'Family',\n",
    "            'Children': 'Children',\n",
    "            'Alone': 'Alone',\n",
    "            'Husband': 'Partner',\n",
    "            'Other relatives': 'Family'\n",
    "        }\n",
    "\n",
    "        print(mapping)\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "    elif(cardinality == 'original'): \n",
    "        target_col = 'household'\n",
    "        mapping = {\n",
    "            'Wife': 'Wife',\n",
    "            'Other Family': 'Family',\n",
    "            'Children': 'Children',\n",
    "            'Alone': 'Alone',\n",
    "            'Husband': 'Husband',\n",
    "            'Other relatives': 'Other'\n",
    "        }\n",
    "\n",
    "        print(mapping)\n",
    "\n",
    "        data[target_col] = data[base_col].map(mapping)\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# 'Base Area' to bin \n",
    "base_col = 'Base Area'\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "try: \n",
    "    if cardinality == 'low': \n",
    "        target_col = 'basearea_fanfoss' # basearea_northbury\n",
    "        target_val = 'Fanfoss'\n",
    "        target_col_alt = 'basearea_northbury'\n",
    "        target_val_alt = 'Northbury'\n",
    "\n",
    "        print('\\nResult:')\n",
    "        data[target_col] = [1 if a == target_val else 0 for a in data[base_col]]\n",
    "\n",
    "        print('\\nAlternative result:')\n",
    "        test = data[['Income', base_col]].copy()\n",
    "        test[target_col_alt] = [1 if a == target_val_alt else 0 for a in test[base_col]]\n",
    "        sns.countplot(data=test, x=target_col_alt, hue='Income')\n",
    "        plt.show()\n",
    "    elif cardinality == 'medium':\n",
    "        data[target_col] = [\n",
    "            target_val if a == target_val \n",
    "            else target_val_alt if a == target_val_alt \n",
    "            else 'Rest' for a in data[base_col]]\n",
    "        cols_to_onehot.append(target_col)\n",
    "\n",
    "    elif cardinality == 'original':\n",
    "        data[target_col] = data[base_col]\n",
    "        cols_to_onehot.append(target_col)\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "    \n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Education Level \n",
    "base_col = 'Education Level'\n",
    "target_col = 'education'\n",
    "print(data.columns)\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "\n",
    "edu_mapping = pd.read_excel(os.path.join(datain_path, 'edu_mapping_2.xlsx'), 'Tabelle2')\n",
    "mapping_options = ['level_0', 'level_1', 'numeric', 'original', 'low']\n",
    "\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'low':\n",
    "        m_option = mapping_options[4]\n",
    "        plot_fct = sns.countplot\n",
    "    elif cardinality == 'medium': \n",
    "        m_option = mapping_options[4]\n",
    "        plot_fct = sns.countplot\n",
    "    elif cardinality == 'high': \n",
    "        m_option = mapping_options[2]\n",
    "        plot_fct = sns.histplot\n",
    "    elif cardinality == 'original':\n",
    "        m_option = mapping_options[3]\n",
    "        plot_fct = sns.countplot       \n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "\n",
    "#print(data[base_col].value_counts())\n",
    "\n",
    "#mapping = dict(edu_mapping[['name', mapping_options[2]]].set_index('name'))\n",
    "#mapping = {k:v for k,v in edu_mapping[['name', mapping_options[2]]].set_index('name').items()}\n",
    "#mapping = edu_mapping[['name', mapping_options[2]]].set_index('name')\n",
    "mapping = edu_mapping[['name', m_option]].rename(columns={m_option: target_col})\n",
    "print(mapping)\n",
    "\n",
    "# drop if reruning the cell \n",
    "if target_col in data.columns: \n",
    "    data.drop(columns=[target_col], inplace=True)\n",
    "\n",
    "data = data.merge(mapping, left_on=base_col, right_on='name', how='left')\n",
    "data.drop(columns=['name'], inplace=True)  \n",
    "\n",
    "# plot target col against prediction classes\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "plot_fct(data=data, x=target_col, hue='Income', ax=ax)\n",
    "#plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)\n",
    "\n",
    "\n",
    "data[[base_col, target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# years of education \n",
    "base_col = 'Years of Education'\n",
    "target_col = 'education_years'\n",
    "data.rename(columns={base_col: target_col}, inplace=True)\n",
    "\n",
    "data.head()\n",
    "#sns.histplot(data=data, y=target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Employment Sector\n",
    "base_col = 'Employment Sector'\n",
    "target_col = 'empl_sector'\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "\n",
    "print(data[base_col].value_counts())\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'deprecated':\n",
    "        mapping = {\n",
    "            'Private Sector - Services ': 'private',\n",
    "            'Self-Employed (Individual)': 'self',\n",
    "            'Public Sector - Others': 'public',\n",
    "            '?': 'unknown',\n",
    "            'Private Sector - Others': 'private',\n",
    "            'Self-Employed (Company)': 'self',\n",
    "            'Public Sector - Government': 'public',\n",
    "            'Unemployed': 'delete',\n",
    "            'Never Worked': 'delete'\n",
    "            }\n",
    "\n",
    "    elif cardinality in ['low', 'medium', 'original']: \n",
    "        mapping = {\n",
    "            'Private Sector - Services ': 'private_services',\n",
    "            'Self-Employed (Individual)': 'self_individual',\n",
    "            'Public Sector - Others': 'public_others',\n",
    "            '?': 'unknown',\n",
    "            'Private Sector - Others': 'private_others',\n",
    "            'Self-Employed (Company)': 'self_company',\n",
    "            'Public Sector - Government': 'public_gov',\n",
    "            'Unemployed': 'unemployed',\n",
    "            'Never Worked': 'unemployed'\n",
    "            }\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "print(mapping)\n",
    "    \n",
    "data[target_col] = data[base_col].map(mapping)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "sns.countplot(data=data, x=target_col, hue='Income', ax=ax)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# role\n",
    "base_col = 'Role'\n",
    "target_col = 'empl_role'\n",
    "\n",
    "plot_feature_imp(data, base_col, weighted=False)\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality in ['low', 'medium']:\n",
    "        mapping = {\n",
    "            'Professor': 'Professor',\n",
    "            'Management': 'Management',\n",
    "            'Repair & constructions': 'Operational_low',\n",
    "            'Administratives': 'Operational',\n",
    "            'Sales': 'Sales',\n",
    "            'Other services': 'Services',\n",
    "            'Machine Operators & Inspectors': 'Operational',\n",
    "            '?': 'unknown',\n",
    "            'Transports': 'Operational_low',\n",
    "            'Cleaners & Handlers': 'Cleaners',\n",
    "            'Agriculture and Fishing': 'Operational',\n",
    "            'IT': 'IT_Security',\n",
    "            'Security': 'IT_Security',\n",
    "            'Household Services': 'Household',\n",
    "            'Army': 'Operational_low'\n",
    "        }\n",
    "    elif cardinality == 'original':       \n",
    "        mapping = {\n",
    "            'Professor': 'Professor',\n",
    "            'Management': 'Management',\n",
    "            'Repair & constructions': 'Constructions',\n",
    "            'Administratives': 'Administratives',\n",
    "            'Sales': 'Sales',\n",
    "            'Other services': 'Services',\n",
    "            'Machine Operators & Inspectors': 'Operator',\n",
    "            '?': 'unknown',\n",
    "            'Transports': 'Transports',\n",
    "            'Cleaners & Handlers': 'Cleaners',\n",
    "            'Agriculture and Fishing': 'Agriculture',\n",
    "            'IT': 'IT', \n",
    "            'Security': 'Security',\n",
    "            'Household Services': 'Household',\n",
    "            'Army': 'Army'\n",
    "        }\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "print(data[base_col].value_counts())\n",
    "\n",
    "print(mapping)\n",
    "    \n",
    "data[target_col] = data[base_col].map(mapping)\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,7))\n",
    "sns.countplot(data=data, x=target_col, hue='Income', ax=ax)\n",
    "plt.show()\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "cols_to_onehot.append(target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "# Working Hours per week\n",
    "base_col = 'Working Hours per week'\n",
    "target_col = 'working_hrs_week'\n",
    "\n",
    "data.rename(columns={base_col: target_col}, inplace=True)\n",
    "\n",
    "sns.histplot(data=data, x=target_col, hue='Income', bins=30)\n",
    "plt.show()\n",
    "\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Money Received\n",
    "base_col = 'Money Received'\n",
    "target_col = 'group_b_received_money'\n",
    "\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'low':\n",
    "        data[target_col] = [1 if v != 0 else 0 for v in data[base_col]]\n",
    "    elif cardinality == 'original':\n",
    "        data[target_col] = data[base_col]\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "#data[[base_col, target_col]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "\n",
    "# Ticket Price\n",
    "base_col = 'Ticket Price'\n",
    "target_col = 'group_c_payed'\n",
    "\n",
    "#low, medium, high, original\n",
    "try: \n",
    "    if cardinality == 'low':\n",
    "        data[target_col] = [1 if v != 0 else 0 for v in data[base_col]]\n",
    "    elif cardinality == 'original':\n",
    "        data[target_col] = data[base_col]\n",
    "    else: \n",
    "        raise NotImplementedError(f'Can not interpret cardinality \"{cardinality}\" for base feature \"{base_col}\"!')\n",
    "except Exception as e:\n",
    "    error_log['cleaning'].append(e)\n",
    "    raise Warning(e)\n",
    "    \n",
    "\n",
    "cols_to_drop.append(base_col)\n",
    "\n",
    "\n",
    "sns.countplot(data=data, x=target_col, hue='Income')\n",
    "plt.show()\n",
    "\n",
    "#data[[base_col, target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "for name, log in error_log.items():\n",
    "    if len(log) > 0: \n",
    "        print(f'{name}:\\n {log}')\n",
    "        raise Warning('Errors occured! See above.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols\n",
    "cols_to_drop.append('CITIZEN_ID')\n",
    "data.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## profile report\n",
    "\n",
    "create_cleaning_report = False\n",
    "if create_cleaning_report: \n",
    "    profile = ProfileReport(\n",
    "        data,\n",
    "        title=f'Cleaned data {dataset_name}' ,\n",
    "        minimal=False, \n",
    "        correlations={\n",
    "        \"pearson\": {\"calculate\": True},\n",
    "        \"spearman\": {\"calculate\": False},\n",
    "        \"kendall\": {\"calculate\": False},\n",
    "        \"phi_k\": {\"calculate\": False},\n",
    "        \"cramers\": {\"calculate\": False},\n",
    "        }\n",
    "    )\n",
    "    profile.to_file(os.path.join(explorations_path, f'profile_data_cleaned_{dataset_name}.html'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target distribution\n",
    "\n",
    "sns.countplot(data=data, x='Income')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Obtain correlation matrix. Round the values to 2 decimal cases. Use the DataFrame corr() and round() method.\n",
    "corr = np.round(data.corr(method=\"pearson\"), decimals=2)\n",
    "\n",
    "# Build annotation matrix (values above |0.5| will appear annotated in the plot)\n",
    "mask_annot = np.absolute(corr.values) >= 0.5\n",
    "annot = np.where(mask_annot, corr.values, np.full(corr.shape,\"\")) # Try to understand what this np.where() does\n",
    "\n",
    "# Plot heatmap of the correlation matrix\n",
    "sns.heatmap(data=corr, annot=annot, cmap=sns.diverging_palette(220, 10, as_cmap=True), \n",
    "            fmt='s', vmin=-1, vmax=1, center=0, square=True, linewidths=.5)\n",
    "\n",
    "# Layout\n",
    "fig.subplots_adjust(top=0.95)\n",
    "fig.suptitle(\"Correlation Matrix\", fontsize=20)\n",
    "\n",
    "plt.savefig(os.path.join(explorations_path, 'correlation_matrix.png'), dpi=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributions \n",
    "\n",
    "ncols = 4\n",
    "n_plots = data.shape[1]\n",
    "nrows = int(np.ceil(n_plots/ncols))\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(ncols=ncols, nrows=nrows, figsize=(15,13))\n",
    "col_no = 0\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols): \n",
    "        if col_no < n_plots:\n",
    "            col = data.columns[col_no]\n",
    "            print(col)\n",
    "            if data[col].dtype in [np.float, np.int]: \n",
    "                sns.histplot(data=data, hue=data.Income, x=col, ax=ax[i,j], bins=30).set_title(col)\n",
    "            else : \n",
    "                sns.countplot(data=data, hue=data.Income, x=col, ax=ax[i,j], dodge=True).set_title(col)\n",
    "            ax[i,j].tick_params(labelrotation=45)\n",
    "            col_no +=1\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig(os.path.join(explorations_path, 'distributions.png'), dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering ideas\n",
    "- age + household: age diff to mean of hh group\n",
    "- \n",
    "\n",
    "### Imputations: \n",
    "- empl_sector == unkown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#one hot encode \n",
    "features = pd.get_dummies(data=data, columns=cols_to_onehot, drop_first=False)\n",
    "features.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep config\n",
    "\n",
    "prep_config = {\n",
    "    'overSampling': True, \n",
    "    'normalize': False,\n",
    "    'outlier': False, # uni- / multivariate, working hours per week\n",
    "    'feature_selection':'boruta'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.copy().drop('Income', axis=1).values\n",
    "y = features.copy().loc[:,'Income'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://github.com/scikit-learn-contrib/boruta_py\n",
    "# https://towardsdatascience.com/feature-selection-with-borutapy-f0ea84c9366\n",
    "\n",
    "\n",
    "# load X and y\n",
    "# NOTE BorutaPy accepts numpy arrays only, hence the .values attribute\n",
    "# X = pd.read_csv('examples/test_X.csv', index_col=0).values\n",
    "# y = pd.read_csv('examples/test_y.csv', header=None, index_col=0).values\n",
    "# y = y.ravel()\n",
    "\n",
    "if prep_config['feature_selection'] == 'boruta': \n",
    "\n",
    "\n",
    "    # define random forest classifier, with utilising all cores and\n",
    "    # sampling in proportion to y labels\n",
    "    rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    "\n",
    "    # define Boruta feature selection method\n",
    "    feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1)\n",
    "\n",
    "    # find all relevant features - 5 features should be selected\n",
    "    feat_selector.fit(X, y)\n",
    "\n",
    "    # check selected features - first 5 features are selected\n",
    "    print(feat_selector.support_)\n",
    "    # check ranking of features\n",
    "    print(feat_selector.ranking_)\n",
    "\n",
    "    # call transform() on X to filter it down to selected features\n",
    "    X_filtered = feat_selector.transform(X)\n",
    "    \n",
    "    X_metadata = pd.DataFrame({\n",
    "    'Features': (features.drop('Income', axis=1).columns.to_list()),\n",
    "    'support': (feat_selector.support_), \n",
    "    'ranking': (feat_selector.ranking_)\n",
    "    })\n",
    "\n",
    "    X_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_metadata.support.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## upcaling to cope with class imbalance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Train test split on updated X\n",
    "X_t, X_val, y_t, y_val = train_test_split(X, y, random_state=42, stratify=None)\n",
    "sns.countplot(y_t)\n",
    "plt.show()\n",
    "\n",
    "if prep_config['overSampling']: \n",
    "    sm = SMOTE(random_state=2, n_jobs=-1, k_neighbors=5, sampling_strategy='auto')\n",
    "    X_t, y_t = sm.fit_sample(X_t, y_t)\n",
    "    sns.countplot(y_t)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###Instantiating Random Forest Classifier\n",
    "rf2 = RandomForestClassifier(n_jobs=-1, n_estimators=500, oob_score=True, max_depth=6, random_state=42)\n",
    "###Fitting Random Forest Classifier to train and test\n",
    "rf2.fit(X_t, y_t)\n",
    "###Predicting on test data\n",
    "y_pred = rf2.predict(X_val)\n",
    "###Test Score\n",
    "rf2.score(X_val, y_val)\n",
    "###Training Score\n",
    "rf2.score(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Instantiating Random Forest Classifier\n",
    "rf2 = RandomForestClassifier(n_jobs=-1, n_estimators=500, oob_score=True, max_depth=6, random_state=42)\n",
    "###Fitting Random Forest Classifier to train and test\n",
    "rf2.fit(X_t, y_t)\n",
    "###Predicting on test data\n",
    "predicted = rf2.predict(X_val)\n",
    "###Test Score\n",
    "rf2.score(X_val, y_val)\n",
    "###Training Score\n",
    "rf2.score(X_t, y_t)\n",
    "\n",
    "print(classification_report(y_val, predicted))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf = GradientBoostingClassifier(\n",
    "    learning_rate=.1,\n",
    "    random_state=0, verbose=False,\n",
    "    subsample=1,\n",
    "    n_estimators=1000, max_depth=3, min_impurity_decrease = 0.1,\n",
    "    max_features='auto',\n",
    "    n_iter_no_change=20)\n",
    "\n",
    "clf.fit(X_t, y_t)\n",
    "clf.predict(X_t[:2])\n",
    "predicted = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, predicted))\n",
    "\n",
    "clf.score(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "\n",
    "def plot_learning_curve(estimator, title, X, y, axes=None, ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate 3 plots: the test and training learning curve, the training\n",
    "    samples vs fit times curve, the fit times vs score curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    axes : array of 3 axes, optional (default=None)\n",
    "        Axes to use for plotting the curves.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "\n",
    "          - None, to use the default 5-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - :term:`CV splitter`,\n",
    "          - An iterable yielding (train, test) splits as arrays of indices.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
    "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
    "        for more details.\n",
    "\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples that will be used to\n",
    "        generate the learning curve. If the dtype is float, it is regarded as a\n",
    "        fraction of the maximum size of the training set (that is determined\n",
    "        by the selected validation method), i.e. it has to be within (0, 1].\n",
    "        Otherwise it is interpreted as absolute sizes of the training sets.\n",
    "        Note that for classification the number of samples usually have to\n",
    "        be big enough to contain at least one sample from each class.\n",
    "        (default: np.linspace(0.1, 1.0, 5))\n",
    "    \"\"\"\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = \\\n",
    "        learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs,\n",
    "                       train_sizes=train_sizes,\n",
    "                       return_times=True)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                         train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                         color=\"r\")\n",
    "    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1,\n",
    "                         color=\"g\")\n",
    "    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "                 label=\"Training score\")\n",
    "    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "                 label=\"Cross-validation score\")\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n",
    "    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n",
    "                         fit_times_mean + fit_times_std, alpha=0.1)\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_times_mean, test_scores_mean, 'o-')\n",
    "    axes[2].fill_between(fit_times_mean, test_scores_mean - test_scores_std,\n",
    "                         test_scores_mean + test_scores_std, alpha=0.1)\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "title = r\"Learning Curves (SVM, RBF kernel, $\\gamma=0.001$)\"\n",
    "# SVC is more expensive so we do a lower number of CV iterations:\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\n",
    "estimator = clf\n",
    "plot_learning_curve(estimator, title, X_t, y_t, ylim=(0.7, 1.01),\n",
    "                    cv=cv, n_jobs=-1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "clf = xgb.XGBClassifier(\n",
    "    use_label_encoder=False, \n",
    "    eval_metric='logloss'\n",
    ")\n",
    "clf.fit(X_t, y_t)\n",
    "predicted = clf.predict(X_val)\n",
    "\n",
    "print(classification_report(y_val, predicted))\n",
    "\n",
    "clf.score(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
